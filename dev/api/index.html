<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · Invertible Networks</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Invertible Networks</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>API Reference</a><ul class="internal"><li><a class="tocitem" href="#Invertible-Layers"><span>Invertible Layers</span></a></li><li><a class="tocitem" href="#Invertible-Networks"><span>Invertible Networks</span></a></li><li><a class="tocitem" href="#Activations-functions"><span>Activations functions</span></a></li><li><a class="tocitem" href="#Dimensions-manipulation"><span>Dimensions manipulation</span></a></li></ul></li><li><a class="tocitem" href="../LICENSE/">LICENSE</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/master/docs/src/api.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h2 id="Invertible-Layers"><a class="docs-heading-anchor" href="#Invertible-Layers">Invertible Layers</a><a id="Invertible-Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Invertible-Layers" title="Permalink"></a></h2><h3 id="Types"><a class="docs-heading-anchor" href="#Types">Types</a><a id="Types-1"></a><a class="docs-heading-anchor-permalink" href="#Types" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.ActNorm" href="#InvertibleNetworks.ActNorm"><code>InvertibleNetworks.ActNorm</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AN = ActNorm(k; logdet=false)</code></pre><p>Create activation normalization layer. The parameters are initialized during  the first use, such that the output has zero mean and unit variance along  channels for the current mini-batch size.</p><p><em>Input</em>:</p><ul><li><p><code>k</code>: number of channels</p></li><li><p><code>logdet</code>: bool to indicate whether to compute the logdet</p></li></ul><p><em>Output</em>:</p><ul><li><code>AN</code>: Network layer for activation normalization.</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Y, logdet = AN.forward(X)</code></p></li><li><p>Inverse mode: <code>X = AN.inverse(Y)</code></p></li><li><p>Backward mode: <code>ΔX, X = AN.backward(ΔY, Y)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>Scaling factor <code>AN.s</code></p></li><li><p>Bias <code>AN.b</code></p></li></ul><p>See also: <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/layers/invertible_layer_actnorm.jl#L9-L41">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.AdditiveCouplingLayerSLIM" href="#InvertibleNetworks.AdditiveCouplingLayerSLIM"><code>InvertibleNetworks.AdditiveCouplingLayerSLIM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">CS = AdditiveCouplingLayerSLIM(nx, ny, n_in, n_hidden, batchsize, Ψ; logdet=false, permute=false, k1=3, k2=3, p1=1, p2=1, s1=1, s2=1)</code></pre><p>Create an invertible additive SLIM coupling layer.</p><p><em>Input</em>: </p><ul><li><code>nx, ny</code>: spatial dimensions of input</li></ul><ul><li><p><code>n_in</code>, <code>n_hidden</code>: number of input and hidden channels</p></li><li><p><code>Ψ</code>: link function</p></li><li><p><code>loget</code>: bool to indicate whether to return the logdet (default is <code>false</code>)</p></li><li><p><code>permute</code>: bool to indicate whether to apply a channel permutation (default is <code>false</code>)</p></li><li><p><code>k1</code>, <code>k2</code>: kernel size of convolutions in residual block. <code>k1</code> is the kernel of the first and third   operator, <code>k2</code> is the kernel size of the second operator</p></li><li><p><code>p1</code>, <code>p2</code>: padding for the first and third convolution (<code>p1</code>) and the second convolution (<code>p2</code>)</p></li><li><p><code>s1</code>, <code>s2</code>: stride for the first and third convolution (<code>s1</code>) and the second convolution (<code>s2</code>)</p></li></ul><p><em>Output</em>:</p><ul><li><code>CS</code>: Invertible SLIM coupling layer</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Y, logdet = CS.forward(X, D, A)</code>    (if constructed with <code>logdet=true</code>)</p></li><li><p>Inverse mode: <code>X = CS.inverse(Y, D, A)</code></p></li><li><p>Backward mode: <code>ΔX, X = CS.backward(ΔY, Y, D, A)</code></p></li><li><p>where <code>A</code> is a linear forward modeling operator and <code>D</code> is the observed data.</p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>None in <code>CL</code> itself</p></li><li><p>Trainable parameters in residual block <code>CL.RB</code> and 1x1 convolution layer <code>CL.C</code></p></li></ul><p>See also: <a href="#InvertibleNetworks.Conv1x1"><code>Conv1x1</code></a>, <a href="#InvertibleNetworks.ResidualBlock"><code>ResidualBlock</code></a>, <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/layers/invertible_layer_slim_additive.jl#L9-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.AffineCouplingLayerSLIM" href="#InvertibleNetworks.AffineCouplingLayerSLIM"><code>InvertibleNetworks.AffineCouplingLayerSLIM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">CS = AffineCouplingLayerSLIM(nx, ny, n_in, n_hidden, batchsize, Ψ; logdet=false, permute=false, k1=3, k2=3, p1=1, p2=1, s1=1, s2=1)</code></pre><p>Create an invertible affine SLIM coupling layer.</p><p><em>Input</em>:</p><ul><li><p><code>nx, ny</code>: spatial dimensions of input</p></li><li><p><code>n_in</code>, <code>n_hidden</code>: number of input and hidden channels</p></li><li><p><code>Ψ</code>: link function</p></li><li><p><code>loget</code>: bool to indicate whether to return the logdet (default is <code>false</code>)</p></li><li><p><code>permute</code>: bool to indicate whether to apply a channel permutation (default is <code>false</code>)</p></li><li><p><code>k1</code>, <code>k2</code>: kernel size of convolutions in residual block. <code>k1</code> is the kernel of the first and third  operator, <code>k2</code> is the kernel size of the second operator</p></li><li><p><code>p1</code>, <code>p2</code>: padding for the first and third convolution (<code>p1</code>) and the second convolution (<code>p2</code>)</p></li><li><p><code>s1</code>, <code>s2</code>: stride for the first and third convolution (<code>s1</code>) and the second convolution (<code>s2</code>)</p></li></ul><p><em>Output</em>:</p><ul><li><code>CS</code>: Invertible SLIM coupling layer</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Y, logdet = CS.forward(X, D, A)</code>    (if constructed with <code>logdet=true</code>)</p></li><li><p>Inverse mode: <code>X = CS.inverse(Y, D, A)</code></p></li><li><p>Backward mode: <code>ΔX, X = CS.backward(ΔY, Y, D, A)</code></p></li><li><p>where <code>A</code> is a linear forward modeling operator and <code>D</code> is the observed data.</p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>None in <code>CL</code> itself</p></li><li><p>Trainable parameters in residual block <code>CL.RB</code> and 1x1 convolution layer <code>CL.C</code></p></li></ul><p>See also: <a href="#InvertibleNetworks.Conv1x1"><code>Conv1x1</code></a>, <a href="#InvertibleNetworks.ResidualBlock"><code>ResidualBlock</code></a>, <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/layers/invertible_layer_slim_affine.jl#L9-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.AffineLayer" href="#InvertibleNetworks.AffineLayer"><code>InvertibleNetworks.AffineLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AL = AffineLayer(nx, ny, nc; logdet=false)</code></pre><p>Create a layer for an affine transformation.</p><p><em>Input</em>:</p><ul><li><p><code>nx</code>, <code>ny,</code>nc`: input dimensions and number of channels</p></li><li><p><code>logdet</code>: bool to indicate whether to compute the logdet</p></li></ul><p><em>Output</em>:</p><ul><li><code>AL</code>: Network layer for affine transformation.</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Y, logdet = AL.forward(X)</code></p></li><li><p>Inverse mode: <code>X = AL.inverse(Y)</code></p></li><li><p>Backward mode: <code>ΔX, X = AL.backward(ΔY, Y)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>Scaling factor <code>AL.s</code></p></li><li><p>Bias <code>AL.b</code></p></li></ul><p>See also: <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/layers/layer_affine.jl#L8-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.ConditionalLayerHINT" href="#InvertibleNetworks.ConditionalLayerHINT"><code>InvertibleNetworks.ConditionalLayerHINT</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">CH = ConditionalLayerHINT(nx, ny, n_in, n_hidden, batchsize; k1=3, k2=3, p1=1, p2=1, s1=1, s2=1, permute=true) (2D)

CH = ConditionalLayerHINT(nx, ny, nz, n_in, n_hidden, batchsize; k1=3, k2=3, p1=1, p2=1, s1=1, s2=1, permute=true) (3D)</code></pre><p>Create a conditional HINT layer based on coupling blocks and 1 level recursion.</p><p><em>Input</em>:</p><ul><li><p><code>nx</code>, <code>ny</code>, <code>nz</code>: spatial dimensions of both <code>X</code> and <code>Y</code>.</p></li><li><p><code>n_in</code>, <code>n_hidden</code>: number of input and hidden channels of both <code>X</code> and <code>Y</code></p></li><li><p><code>k1</code>, <code>k2</code>: kernel size of convolutions in residual block. <code>k1</code> is the kernel of the first and third  operator, <code>k2</code> is the kernel size of the second operator.</p></li><li><p><code>p1</code>, <code>p2</code>: padding for the first and third convolution (<code>p1</code>) and the second convolution (<code>p2</code>)</p></li><li><p><code>s1</code>, <code>s2</code>: stride for the first and third convolution (<code>s1</code>) and the second convolution (<code>s2</code>)</p></li><li><p><code>permute</code>: bool to indicate whether to permute <code>X</code> and <code>Y</code>. Default is <code>true</code></p></li></ul><p><em>Output</em>:</p><ul><li><code>CH</code>: Conditional HINT coupling layer.</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Zx, Zy, logdet = CH.forward_X(X, Y)</code></p></li><li><p>Inverse mode: <code>X, Y = CH.inverse(Zx, Zy)</code></p></li><li><p>Backward mode: <code>ΔX, ΔY, X, Y = CH.backward(ΔZx, ΔZy, Zx, Zy)</code></p></li><li><p>Forward mode Y: <code>Zy = CH.forward_Y(Y)</code></p></li><li><p>Inverse mode Y: <code>Y = CH.inverse(Zy)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>None in <code>CH</code> itself</p></li><li><p>Trainable parameters in coupling layers <code>CH.CL_X</code>, <code>CH.CL_Y</code>, <code>CH.CL_YX</code> and in permutation layers <code>CH.C_X</code> and <code>CH.C_Y</code>.</p></li></ul><p>See also: <a href="#InvertibleNetworks.CouplingLayerBasic"><code>CouplingLayerBasic</code></a>, <a href="#InvertibleNetworks.ResidualBlock"><code>ResidualBlock</code></a>, <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/conditional_layers/conditional_layer_hint.jl#L7-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.ConditionalLayerSLIM" href="#InvertibleNetworks.ConditionalLayerSLIM"><code>InvertibleNetworks.ConditionalLayerSLIM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">CI = ConditionalLayerSLIM(nx1, nx2, nx_in, nx_hidden, ny1, ny2, ny_in, ny_hidden, batchsize, Op;
    type=&quot;affine&quot;, k1=3, k2=3, p1=1, p2=1, s1=1, s2=1)</code></pre><p>Create a conditional SLIM layer based on the HINT architecture.</p><p><em>Input</em>: </p><ul><li><p><code>nx1</code>, <code>nx2</code>: spatial dimensions of <code>X</code></p></li><li><p><code>nx_in</code>, <code>nx_hidden</code>: number of input and hidden channels of <code>X</code></p></li><li><p><code>ny1</code>, <code>ny2</code>: spatial dimensions of <code>Y</code></p></li></ul><ul><li><p><code>ny_in</code>, <code>ny_hidden</code>: number of input and hidden channels of <code>Y</code></p></li><li><p><code>Op</code>: Linear forward modeling operator</p></li><li><p><code>type</code>: string to indicate which type of data coupling layer to use (<code>&quot;additive&quot;</code>, <code>&quot;affine&quot;</code>, <code>&quot;learned&quot;</code>)</p></li><li><p><code>k1</code>, <code>k2</code>: kernel size of convolutions in residual block. <code>k1</code> is the kernel of the first and third   operator, <code>k2</code> is the kernel size of the second operator.</p></li><li><p><code>p1</code>, <code>p2</code>: padding for the first and third convolution (<code>p1</code>) and the second convolution (<code>p2</code>)</p></li><li><p><code>s1</code>, <code>s2</code>: stride for the first and third convolution (<code>s1</code>) and the second convolution (<code>s2</code>)</p></li></ul><p><em>Output</em>:</p><ul><li><code>CI</code>: Conditional SLIM coupling layer.</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Zx, Zy, logdet = CI.forward_X(X, Y, Op)</code></p></li><li><p>Inverse mode: <code>X, Y = CI.inverse(Zx, Zy, Op)</code></p></li><li><p>Backward mode: <code>ΔX, ΔY, X, Y = CI.backward(ΔZx, ΔZy, Zx, Zy, Op)</code></p></li><li><p>Forward mode Y: <code>Zy = CI.forward_Y(Y)</code></p></li><li><p>Inverse mode Y: <code>Y = CI.inverse(Zy)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>None in <code>CI</code> itself</p></li><li><p>Trainable parameters in coupling layers <code>CI.CL_X</code>, <code>CI.CL_Y</code>, <code>CI.CL_XY</code> and in permutation layers <code>CI.C_X</code> and <code>CI.C_Y</code>.</p></li></ul><p>See also: <a href="#InvertibleNetworks.CouplingLayerHINT"><code>CouplingLayerHINT</code></a>, <a href="#InvertibleNetworks.AffineCouplingLayerSLIM"><code>AffineCouplingLayerSLIM</code></a>, <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/conditional_layers/conditional_layer_slim.jl#L7-L58">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.ConditionalResidualBlock" href="#InvertibleNetworks.ConditionalResidualBlock"><code>InvertibleNetworks.ConditionalResidualBlock</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RB = ConditionalResidualBlock(nx1, nx2, nx_in, ny1, ny2, ny_in, n_hidden, batchsize; k1=3, k2=3, p1=1, p2=1, s1=1, s2=1)</code></pre><p>Create a (non-invertible) conditional residual block, consisting of one dense and three convolutional layers  with ReLU activation functions. The dense operator maps the data to the image space and both tensors are  concatenated and fed to the subsequent convolutional layers.</p><p><em>Input</em>:</p><ul><li><p><code>nx1</code>, <code>nx2</code>, <code>nx_in</code>: spatial dimensions and no. of channels of input image</p></li><li><p><code>ny1</code>, <code>ny2</code>, <code>ny_in</code>: spatial dimensions and no. of channels of input data</p></li><li><p><code>n_hidden</code>: number of hidden channels</p></li><li><p><code>k1</code>, <code>k2</code>: kernel size of convolutions in residual block. <code>k1</code> is the kernel of the first and third  operator, <code>k2</code> is the kernel size of the second operator.</p></li><li><p><code>p1</code>, <code>p2</code>: padding for the first and third convolution (<code>p1</code>) and the second convolution (<code>p2</code>)</p></li><li><p><code>s1</code>, <code>s2</code>: strides for the first and third convolution (<code>s1</code>) and the second convolution (<code>s2</code>)</p></li></ul><p>or</p><p><em>Output</em>:</p><ul><li><code>RB</code>: conditional residual block layer</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Zx, Zy = RB.forward(X, Y)</code></p></li><li><p>Backward mode: <code>ΔX, ΔY = RB.backward(ΔZx, ΔZy, X, Y)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>Convolutional kernel weights <code>RB.W0</code>, <code>RB.W1</code>, <code>RB.W2</code> and <code>RB.W3</code></p></li><li><p>Bias terms <code>RB.b0</code>, <code>RB.b1</code> and <code>RB.b2</code></p></li></ul><p>See also: <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/conditional_layers/conditional_layer_residual_block.jl#L7-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.Conv1x1" href="#InvertibleNetworks.Conv1x1"><code>InvertibleNetworks.Conv1x1</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">C = Conv1x1(k; logdet=false)</code></pre><p>or</p><pre><code class="language-none">C = Conv1x1(v1, v2, v3; logdet=false)</code></pre><p>Create network layer for 1x1 convolutions using Householder reflections.</p><p><em>Input</em>:</p><ul><li><p><code>k</code>: number of channels</p></li><li><p><code>v1</code>, <code>v2</code>, <code>v3</code>: Vectors from which to construct matrix.</p></li><li><p><code>logdet</code>: if true, returns logdet in forward pass (which is always zero)</p></li></ul><p><em>Output</em>:</p><ul><li><code>C</code>: Network layer for 1x1 convolutions with Householder reflections.</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Y, logdet = C.forward(X)</code></p></li><li><p>Backward mode: <code>ΔX, X = C.backward((ΔY, Y))</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li>Householder vectors <code>C.v1</code>, <code>C.v2</code>, <code>C.v3</code></li></ul><p>See also: <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/layers/invertible_layer_conv1x1.jl#L9-L41">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.CouplingLayerBasic" href="#InvertibleNetworks.CouplingLayerBasic"><code>InvertibleNetworks.CouplingLayerBasic</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">CL = CouplingLayerBasic(RB::ResidualBlock; logdet=false)</code></pre><p>or</p><pre><code class="language-none">CL = CouplingLayerBasic(nx, ny, n_in, n_hidden, batchsize; k1=3, k2=3, p1=1, p2=1, s1=1, s2=1, logdet=false) (2D)

CL = CouplingLayerBasic(nx, ny, nz, n_in, n_hidden, batchsize; k1=3, k2=3, p1=1, p2=1, s1=1, s2=1, logdet=false) (3D)</code></pre><p>Create a Real NVP-style invertible coupling layer with a residual block.</p><p><em>Input</em>:</p><ul><li><p><code>RB::ResidualBlock</code>: residual block layer consisting of 3 convolutional layers with ReLU activations.</p></li><li><p><code>logdet</code>: bool to indicate whether to compte the logdet of the layer</p></li></ul><p>or</p><ul><li><p><code>nx</code>, <code>ny</code>, <code>nz</code>: spatial dimensions of input</p></li><li><p><code>n_in</code>, <code>n_hidden</code>: number of input and hidden channels</p></li><li><p><code>k1</code>, <code>k2</code>: kernel size of convolutions in residual block. <code>k1</code> is the kernel of the first and third  operator, <code>k2</code> is the kernel size of the second operator.</p></li><li><p><code>p1</code>, <code>p2</code>: padding for the first and third convolution (<code>p1</code>) and the second convolution (<code>p2</code>)</p></li><li><p><code>s1</code>, <code>s2</code>: stride for the first and third convolution (<code>s1</code>) and the second convolution (<code>s1</code>)</p></li></ul><p><em>Output</em>:</p><ul><li><code>CL</code>: Invertible Real NVP coupling layer.</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Y1, Y2, logdet = CL.forward(X1, X2)</code>    (if constructed with <code>logdet=true</code>)</p></li><li><p>Inverse mode: <code>X1, X2 = CL.inverse(Y1, Y2)</code></p></li><li><p>Backward mode: <code>ΔX1, ΔX2, X1, X2 = CL.backward(ΔY1, ΔY2, Y1, Y2)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>None in <code>CL</code> itself</p></li><li><p>Trainable parameters in residual block <code>CL.RB</code></p></li></ul><p>See also: <a href="#InvertibleNetworks.ResidualBlock"><code>ResidualBlock</code></a>, <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/layers/invertible_layer_basic.jl#L9-L58">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.CouplingLayerGlow" href="#InvertibleNetworks.CouplingLayerGlow"><code>InvertibleNetworks.CouplingLayerGlow</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">CL = CouplingLayerGlow(C::Conv1x1, RB::ResidualBlock; logdet=false)</code></pre><p>or</p><pre><code class="language-none">CL = CouplingLayerGlow(nx, ny, n_in, n_hidden, batchsize; k1=3, k2=1, p1=1, p2=0, s1=1, s2=1, logdet=false)</code></pre><p>Create a Real NVP-style invertible coupling layer based on 1x1 convolutions and a residual block.</p><p><em>Input</em>:</p><ul><li><p><code>C::Conv1x1</code>: 1x1 convolution layer</p></li><li><p><code>RB::ResidualBlock</code>: residual block layer consisting of 3 convolutional layers with ReLU activations.</p></li><li><p><code>logdet</code>: bool to indicate whether to compte the logdet of the layer</p></li></ul><p>or</p><ul><li><p><code>nx, ny</code>: spatial dimensions of input</p></li><li><p><code>n_in</code>, <code>n_hidden</code>: number of input and hidden channels</p></li><li><p><code>k1</code>, <code>k2</code>: kernel size of convolutions in residual block. <code>k1</code> is the kernel of the first and third  operator, <code>k2</code> is the kernel size of the second operator.</p></li><li><p><code>p1</code>, <code>p2</code>: padding for the first and third convolution (<code>p1</code>) and the second convolution (<code>p2</code>)</p></li><li><p><code>s1</code>, <code>s2</code>: stride for the first and third convolution (<code>s1</code>) and the second convolution (<code>s2</code>)</p></li></ul><p><em>Output</em>:</p><ul><li><code>CL</code>: Invertible Real NVP coupling layer.</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Y, logdet = CL.forward(X)</code>    (if constructed with <code>logdet=true</code>)</p></li><li><p>Inverse mode: <code>X = CL.inverse(Y)</code></p></li><li><p>Backward mode: <code>ΔX, X = CL.backward(ΔY, Y)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>None in <code>CL</code> itself</p></li><li><p>Trainable parameters in residual block <code>CL.RB</code> and 1x1 convolution layer <code>CL.C</code></p></li></ul><p>See also: <a href="#InvertibleNetworks.Conv1x1"><code>Conv1x1</code></a>, <a href="#InvertibleNetworks.ResidualBlock"><code>ResidualBlock</code></a>, <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/layers/invertible_layer_glow.jl#L9-L58">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.CouplingLayerHINT" href="#InvertibleNetworks.CouplingLayerHINT"><code>InvertibleNetworks.CouplingLayerHINT</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">H = CouplingLayerHINT(nx, ny, n_in, n_hidden, batchsize;
    logdet=false, permute=&quot;none&quot;, k1=3, k2=3, p1=1, p2=1, s1=1, s2=1) (2D)

H = CouplingLayerHINT(nx, ny, nz, n_in, n_hidden, batchsize;
    logdet=false, permute=&quot;none&quot;, k1=3, k2=3, p1=1, p2=1, s1=1, s2=1) (3D)</code></pre><p>Create a recursive HINT-style invertible layer based on coupling blocks.</p><p><em>Input</em>:</p><ul><li><p><code>nx</code>, <code>ny</code>, <code>nz</code>: spatial dimensions of input</p></li><li><p><code>n_in</code>, <code>n_hidden</code>: number of input and hidden channels</p></li><li><p><code>logdet</code>: bool to indicate whether to return the log determinant. Default is <code>false</code>.</p></li><li><p><code>permute</code>: string to specify permutation. Options are <code>&quot;none&quot;</code>, <code>&quot;lower&quot;</code>, <code>&quot;both&quot;</code> or <code>&quot;full&quot;</code>.</p></li><li><p><code>k1</code>, <code>k2</code>: kernel size of convolutions in residual block. <code>k1</code> is the kernel of the first and third  operator, <code>k2</code> is the kernel size of the second operator.</p></li><li><p><code>p1</code>, <code>p2</code>: padding for the first and third convolution (<code>p1</code>) and the second convolution (<code>p2</code>)</p></li><li><p><code>s1</code>, <code>s2</code>: stride for the first and third convolution (<code>s1</code>) and the second convolution (<code>s2</code>)</p></li></ul><p><em>Output</em>:</p><ul><li><code>H</code>: Recursive invertible HINT coupling layer.</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Y = H.forward(X)</code></p></li><li><p>Inverse mode: <code>X = H.inverse(Y)</code></p></li><li><p>Backward mode: <code>ΔX, X = H.backward(ΔY, Y)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>None in <code>H</code> itself</p></li><li><p>Trainable parameters in coupling layers <code>H.CL</code></p></li></ul><p>See also: <a href="#InvertibleNetworks.CouplingLayerBasic"><code>CouplingLayerBasic</code></a>, <a href="#InvertibleNetworks.ResidualBlock"><code>ResidualBlock</code></a>, <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/layers/invertible_layer_hint.jl#L7-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.CouplingLayerIRIM" href="#InvertibleNetworks.CouplingLayerIRIM"><code>InvertibleNetworks.CouplingLayerIRIM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">IL = CouplingLayerIRIM(C::Conv1x1, RB::ResidualBlock)</code></pre><p>or</p><pre><code class="language-none">IL = CouplingLayerIRIM(nx, ny, n_in, n_hidden, batchsize; k1=4, k2=3, p1=0, p2=1, s1=4, s2=1, logdet=false) (2D)

IL = CouplingLayerIRIM(nx, ny, nz, n_in, n_hidden, batchsize; k1=4, k2=3, p1=0, p2=1, s1=4, s2=1, logdet=false) (3D)</code></pre><p>Create an i-RIM invertible coupling layer based on 1x1 convolutions and a residual block. </p><p><em>Input</em>: </p><ul><li><code>C::Conv1x1</code>: 1x1 convolution layer</li></ul><ul><li><code>RB::ResidualBlock</code>: residual block layer consisting of 3 convolutional layers with ReLU activations.</li></ul><p>or</p><ul><li><code>nx</code>, <code>ny</code>, <code>nz</code>: spatial dimensions of input</li></ul><ul><li><p><code>n_in</code>, <code>n_hidden</code>: number of input and hidden channels</p></li><li><p><code>k1</code>, <code>k2</code>: kernel size of convolutions in residual block. <code>k1</code> is the kernel of the first and third   operator, <code>k2</code> is the kernel size of the second operator.</p></li><li><p><code>p1</code>, <code>p2</code>: padding for the first and third convolution (<code>p1</code>) and the second convolution (<code>p2</code>)</p></li><li><p><code>s1</code>, <code>s2</code>: stride for the first and third convolution (<code>s1</code>) and the second convolution (<code>s2</code>)</p></li></ul><p><em>Output</em>:</p><ul><li><code>IL</code>: Invertible i-RIM coupling layer.</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Y = IL.forward(X)</code></p></li><li><p>Inverse mode: <code>X = IL.inverse(Y)</code></p></li><li><p>Backward mode: <code>ΔX, X = IL.backward(ΔY, Y)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>None in <code>IL</code> itself</p></li><li><p>Trainable parameters in residual block <code>IL.RB</code> and 1x1 convolution layer <code>IL.C</code></p></li></ul><p>See also: <a href="#InvertibleNetworks.Conv1x1"><code>Conv1x1</code></a>, <a href="@ref"><code>ResidualBlock!</code></a>, <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/layers/invertible_layer_irim.jl#L7-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.FluxBlock" href="#InvertibleNetworks.FluxBlock"><code>InvertibleNetworks.FluxBlock</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">FB = FluxBlock(model::Chain)</code></pre><p>Create a (non-invertible) neural network block from a Flux network.</p><p><em>Input</em>: </p><ul><li><code>model</code>: Flux neural network of type <code>Chain</code></li></ul><p><em>Output</em>:</p><ul><li><code>FB</code>: residual block layer</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Y = FB.forward(X)</code></p></li><li><p>Backward mode: <code>ΔX = FB.backward(ΔY, X)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li>Network parameters given by <code>Flux.parameters(model)</code></li></ul><p>See also:  <a href="@ref"><code>Chain</code></a>, <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/layers/layer_flux_block.jl#L7-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.HyperbolicLayer" href="#InvertibleNetworks.HyperbolicLayer"><code>InvertibleNetworks.HyperbolicLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">HyperbolicLayer(nx, ny, n_in, batchsize, kernel, stride, pad; action=0, α=1f0, n_hidden=1)</code></pre><p>or</p><pre><code class="language-none">HyperbolicLayer(W, b, nx, ny, batchsize, stride, pad; action=0, α=1f0)</code></pre><p>Create an invertible hyperbolic coupling layer.</p><p><em>Input</em>:</p><ul><li><p><code>nx</code>, <code>ny</code>, <code>n_in</code>, <code>batchsize</code>: Dimensions of input tensor</p></li><li><p><code>kernel</code>, <code>stride</code>, <code>pad</code>: Kernel size, stride and padding of the convolutional operator</p></li><li><p><code>action</code>: String that defines whether layer keeps the number of channels fixed (<code>0</code>),  increases it by a factor of 4 (or 8 in 3D) (<code>1</code>) or decreased it by a factor of 4 (or 8) (<code>-1</code>).</p></li><li><p><code>W</code>, <code>b</code>: Convolutional weight and bias. <code>W</code> has dimensions of <code>(kernel, kernel, n_in, n_in)</code>. <code>b</code> has dimensions of <code>n_in</code>.</p></li><li><p><code>α</code>: Step size for second time derivative. Default is 1.</p></li><li><p><code>n_hidden</code>: Increase the no. of channels by <code>n_hidden</code> in the forward convolution.  After applying the transpose convolution, the dimensions are back to the input dimensions.</p></li></ul><p><em>Output</em>:</p><ul><li><code>HL</code>: Invertible hyperbolic coupling layer</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>X_curr, X_new = HL.forward(X_prev, X_curr)</code></p></li><li><p>Inverse mode: <code>X_prev, X_curr = HL.inverse(X_curr, X_new)</code></p></li><li><p>Backward mode: <code>ΔX_prev, ΔX_curr, X_prev, X_curr = HL.backward(ΔX_curr, ΔX_new, X_curr, X_new)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p><code>HL.W</code>: Convolutional kernel</p></li><li><p><code>HL.b</code>: Bias</p></li></ul><p>See also: <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/layers/invertible_layer_hyperbolic.jl#L7-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.LearnedCouplingLayerSLIM" href="#InvertibleNetworks.LearnedCouplingLayerSLIM"><code>InvertibleNetworks.LearnedCouplingLayerSLIM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">CS = LearnedCouplingLayerSLIM(nx1, nx2, nx_in, ny1, ny2, ny_in, n_hidden, batchsize; 
    logdet::Bool=false, permute::Bool=false, k1=3, k2=3, p1=1, p2=1, s1=1, s2=1)</code></pre><p>Create an invertible SLIM coupling layer with a learned data-to-image-space map.</p><p><em>Input</em>: </p><ul><li><code>nx1</code>, <code>nx2</code>, <code>nx_in</code>: spatial dimensions and no. of channels of input image</li></ul><ul><li><p><code>ny1</code>, <code>ny2</code>, <code>ny_in</code>: spatial dimensions and no. of channels of input data</p></li><li><p><code>n_hidden</code>: number of hidden units in conditional residual block</p></li><li><p><code>loget</code>: bool to indicate whether to return the logdet (default is <code>false</code>)</p></li><li><p><code>permute</code>: bool to indicate whether to apply a channel permutation (default is <code>false</code>)</p></li><li><p><code>k1</code>, <code>k2</code>: kernel size of convolutions in residual block. <code>k1</code> is the kernel of the first and third   operator, <code>k2</code> is the kernel size of the second operator</p></li><li><p><code>p1</code>, <code>p2</code>: padding for the first and third convolution (<code>p1</code>) and the second convolution (<code>p2</code>)</p></li><li><p><code>s1</code>, <code>s2</code>: stride for the first and third convolution (<code>s1</code>) and the second convolution (<code>s2</code>)</p></li></ul><p><em>Output</em>:</p><ul><li><code>CS</code>: Invertible SLIM coupling layer with learned data-to-image map</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Y, logdet = CS.forward(X, D, A)</code>    (if constructed with <code>logdet=true</code>)</p></li><li><p>Inverse mode: <code>X = CS.inverse(Y, D, A)</code></p></li><li><p>Backward mode: <code>ΔX, X = CS.backward(ΔY, Y, D, A)</code></p></li><li><p>where <code>A</code> is a linear forward modeling operator and <code>D</code> is the observed data.</p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>None in <code>CL</code> itself</p></li><li><p>Trainable parameters in residual block <code>CL.RB</code> and 1x1 convolution layer <code>CL.C</code></p></li></ul><p>See also: <a href="#InvertibleNetworks.Conv1x1"><code>Conv1x1</code></a>, <a href="#InvertibleNetworks.ResidualBlock"><code>ResidualBlock</code></a>, <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/layers/invertible_layer_slim_learned.jl#L9-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.ResidualBlock" href="#InvertibleNetworks.ResidualBlock"><code>InvertibleNetworks.ResidualBlock</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RB = ResidualBlock(nx, ny, n_in, n_hidden, batchsize; k1=3, k2=3, p1=1, p2=1, s1=1, s2=1, fan=false) (2D)

RB = ResidualBlock(nx, ny, nz, n_in, n_hidden, batchsize; k1=3, k2=3, p1=1, p2=1, s1=1, s2=1, fan=false) (3D)</code></pre><p>or</p><pre><code class="language-none">RB = ResidualBlock(nx, ny, n_in, n_hidden, batchsize; k1=3, k2=3, p1=1, p2=1, s1=1, s2=1, fan=false) (2D)

RB = ResidualBlock(nx, ny, nz, n_in, n_hidden, batchsize; k1=3, k2=3, p1=1, p2=1, s1=1, s2=1, fan=false) (3D)</code></pre><p>Create a (non-invertible) residual block, consisting of three convolutional layers and activation functions.  The first convolution is a downsampling operation with a stride equal to the kernel dimension. The last  convolution is the corresponding transpose operation and upsamples the data to either its original dimensions  or to twice the number of input channels (for <code>fan=true</code>). The first and second layer contain a bias term.</p><p><em>Input</em>:</p><ul><li><p><code>nx</code>, <code>ny</code>, <code>nz</code>: spatial dimensions of input</p></li><li><p><code>n_in</code>, <code>n_hidden</code>: number of input and hidden channels</p></li><li><p><code>k1</code>, <code>k2</code>: kernel size of convolutions in residual block. <code>k1</code> is the kernel of the first and third  operator, <code>k2</code> is the kernel size of the second operator.</p></li><li><p><code>p1</code>, <code>p2</code>: padding for the first and third convolution (<code>p1</code>) and the second convolution (<code>p2</code>)</p></li><li><p><code>s1</code>, <code>s2</code>: stride for the first and third convolution (<code>s1</code>) and the second convolution (<code>s2</code>)</p></li><li><p><code>fan</code>: bool to indicate whether the ouput has twice the number of input channels. For <code>fan=false</code>, the last  activation function is a gated linear unit (thereby bringing the output back to the original dimensions).  For <code>fan=true</code>, the last activation is a ReLU, in which case the output has twice the number of channels  as the input.</p></li></ul><p>or</p><ul><li><p><code>W1</code>, <code>W2</code>, <code>W3</code>: 4D tensors of convolutional weights</p></li><li><p><code>b1</code>, <code>b2</code>: bias terms</p></li><li><p><code>nx</code>, <code>ny</code>: spatial dimensions of input image</p></li></ul><p><em>Output</em>:</p><ul><li><code>RB</code>: residual block layer</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Y = RB.forward(X)</code></p></li><li><p>Backward mode: <code>ΔX = RB.backward(ΔY, X)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>Convolutional kernel weights <code>RB.W1</code>, <code>RB.W2</code> and <code>RB.W3</code></p></li><li><p>Bias terms <code>RB.b1</code> and <code>RB.b2</code></p></li></ul><p>See also: <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/layers/layer_residual_block.jl#L7-L66">source</a></section></article><h2 id="Invertible-Networks"><a class="docs-heading-anchor" href="#Invertible-Networks">Invertible Networks</a><a id="Invertible-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Invertible-Networks" title="Permalink"></a></h2><h3 id="Types-2"><a class="docs-heading-anchor" href="#Types-2">Types</a><a class="docs-heading-anchor-permalink" href="#Types-2" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.NetworkConditionalHINT" href="#InvertibleNetworks.NetworkConditionalHINT"><code>InvertibleNetworks.NetworkConditionalHINT</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">CH = NetworkConditionalHINT(nx, ny, n_in, batchsize, n_hidden, depth; k1=3, k2=3, p1=1, p2=1, s1=1, s2=1)</code></pre><p>Create a conditional HINT network for data-driven generative modeling based  on the change of variables formula.</p><p><em>Input</em>:</p><ul><li><p><code>nx</code>, <code>ny</code>, <code>n_in</code>, <code>batchsize</code>: spatial dimensions, number of channels and batchsize of input tensors <code>X</code> and <code>Y</code></p></li><li><p><code>n_hidden</code>: number of hidden units in residual blocks</p></li><li><p><code>depth</code>: number network layers</p></li><li><p><code>k1</code>, <code>k2</code>: kernel size for first and third residual layer (<code>k1</code>) and second layer (<code>k2</code>)</p></li><li><p><code>p1</code>, <code>p2</code>: respective padding sizes for residual block layers</p></li><li><p><code>s1</code>, <code>s2</code>: respective strides for residual block layers</p></li></ul><p><em>Output</em>:</p><ul><li><code>CH</code>: conditioinal HINT network</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Zx, Zy, logdet = CH.forward(X, Y)</code></p></li><li><p>Inverse mode: <code>X, Y = CH.inverse(Zx, Zy)</code></p></li><li><p>Backward mode: <code>ΔX, X = CH.backward(ΔZx, ΔZy, Zx, Zy)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>None in <code>CH</code> itself</p></li><li><p>Trainable parameters in activation normalizations <code>CH.AN_X[i]</code> and <code>CH.AN_Y[i]</code>,</p></li></ul><p>and in coupling layers <code>CH.CL[i]</code>, where <code>i</code> ranges from <code>1</code> to <code>depth</code>.</p><p>See also: <a href="#InvertibleNetworks.ActNorm"><code>ActNorm</code></a>, <a href="@ref"><code>ConditionalLayerHINT!</code></a>, <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/networks/invertible_network_conditional_hint.jl#L7-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.NetworkGlow" href="#InvertibleNetworks.NetworkGlow"><code>InvertibleNetworks.NetworkGlow</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">G = NetworkGlow(nx, ny, n_in, batchsize, n_hidden, L, K; k1=3, k2=1, p1=1, p2=0, s1=1, s2=1)</code></pre><p>Create an invertible network based on the Glow architecture. Each flow step in the inner loop   consists of an activation normalization layer, followed by an invertible coupling layer with  1x1 convolutions and a residual block. The outer loop performs a squeezing operation prior   to the inner loop, and a splitting operation afterwards.</p><p><em>Input</em>: </p><ul><li><code>nx</code>, <code>ny</code>, <code>n_in</code>, <code>batchsize</code>: spatial dimensions, number of channels and batchsize of input tensor</li></ul><ul><li><p><code>n_hidden</code>: number of hidden units in residual blocks</p></li><li><p><code>L</code>: number of scales (outer loop)</p></li><li><p><code>K</code>: number of flow steps per scale (inner loop)</p></li><li><p><code>k1</code>, <code>k2</code>: kernel size of convolutions in residual block. <code>k1</code> is the kernel of the first and third </p></li></ul><p>operator, <code>k2</code> is the kernel size of the second operator.</p><ul><li><p><code>p1</code>, <code>p2</code>: padding for the first and third convolution (<code>p1</code>) and the second convolution (<code>p2</code>)</p></li><li><p><code>s1</code>, <code>s2</code>: stride for the first and third convolution (<code>s1</code>) and the second convolution (<code>s2</code>)</p></li></ul><p><em>Output</em>:</p><ul><li><code>G</code>: invertible Glow network.</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Y, logdet = G.forward(X)</code></p></li><li><p>Backward mode: <code>ΔX, X = G.backward(ΔY, Y)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>None in <code>G</code> itself</p></li><li><p>Trainable parameters in activation normalizations <code>G.AN[i,j]</code> and coupling layers <code>G.C[i,j]</code>, where <code>i</code> and <code>j</code> range from <code>1</code> to <code>L</code> and <code>K</code> respectively.</p></li></ul><p>See also: <a href="#InvertibleNetworks.ActNorm"><code>ActNorm</code></a>, <a href="@ref"><code>CouplingLayerGlow!</code></a>, <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/networks/invertible_network_glow.jl#L8-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.NetworkHyperbolic" href="#InvertibleNetworks.NetworkHyperbolic"><code>InvertibleNetworks.NetworkHyperbolic</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">H = NetworkHyperbolic(nx, ny, n_in, batchsize, architecture; k=3, s=1, p=1, logdet=true, α=1f0)

H = NetworkHyperbolic(nx, ny, nz, n_in, batchsize, architecture; k=3, s=1, p=1, logdet=true, α=1f0)</code></pre><p>Create an invertible network based on hyperbolic layers. The network architecture is specified by a tuple  of the form ((action<em>1, n</em>hidden<em>1), (action</em>2, n<em>hidden</em>2), ... ). Each inner tuple corresonds to an additional layer.   The first inner tuple argument specifies whether the respective layer increases the number of channels (set to <code>1</code>),   decreases it (set to <code>-1</code>) or leaves it constant (set to <code>0</code>).  The second argument specifies the number of hidden   units for that layer.</p><p><em>Input</em>: </p><ul><li><code>nx</code>, <code>ny</code>, <code>nz</code>, <code>n_in</code>, <code>batchsize</code>: spatial dimensions, number of channels and batchsize of input tensor. <code>nz</code> is optional.</li></ul><ul><li><p><code>n_hidden</code>: number of hidden units in residual blocks</p></li><li><p><code>architecture</code>: Tuple of tuples specifying the network architecture; ((action<em>1, n</em>hidden<em>1), (action</em>2, n<em>hidden</em>2))</p></li><li><p><code>k</code>, <code>s</code>, <code>p</code>: Kernel size, stride and padding of convolutional kernels</p></li></ul><ul><li><p><code>logdet</code>: Bool to indicate whether to return the logdet</p></li><li><p><code>α</code>: Step size in hyperbolic network. Defaults to <code>1</code></p></li></ul><p><em>Output</em>:</p><ul><li><code>H</code>: invertible hyperbolic network.</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Y_prev, Y_curr, logdet = H.forward(X_prev, X_curr)</code></p></li><li><p>Inverse mode: <code>X_curr, X_new = H.inverse(Y_curr, Y_new)</code></p></li><li><p>Backward mode: <code>ΔX_curr, ΔX_new, X_curr, X_new = H.backward(ΔY_curr, ΔY_new, Y_curr, Y_new)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>None in <code>H</code> itself</p></li><li><p>Trainable parameters in the hyperbolic layers <code>H.HL[j]</code>.</p></li></ul><p>See also: <a href="@ref"><code>CouplingLayer!</code></a>, <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/networks/invertible_network_hyperbolic.jl#L7-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.NetworkLoop" href="#InvertibleNetworks.NetworkLoop"><code>InvertibleNetworks.NetworkLoop</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">L = NetworkLoop(nx, ny, n_in, n_hidden, batchsize, maxiter, Ψ; k1=4, k2=3, p1=0, p2=1, s1=4, s2=1) (2D)

L = NetworkLoop(nx, ny, nz, n_in, n_hidden, batchsize, maxiter, Ψ; k1=4, k2=3, p1=0, p2=1, s1=4, s2=1) (3D)</code></pre><p>Create an invertibel recurrent inference machine (i-RIM) consisting of an unrooled loop  for a given number of iterations.</p><p><em>Input</em>: </p><ul><li><code>nx</code>, <code>ny</code>, <code>nz</code>, <code>n_in</code>, <code>batchsize</code>: spatial dimensions, number of channels and batchsize of input tensor</li></ul><ul><li><p><code>n_hidden</code>: number of hidden units in residual blocks</p></li><li><p><code>maxiter</code>: number unrolled loop iterations</p></li><li><p><code>Ψ</code>: link function</p></li><li><p><code>k1</code>, <code>k2</code>: stencil sizes for convolutions in the residual blocks. The first convolution  uses a stencil of size and stride <code>k1</code>, thereby downsampling the input. The second  convolutions uses a stencil of size <code>k2</code>. The last layer uses a stencil of size and stride <code>k1</code>, but performs the transpose operation of the first convolution, thus upsampling the output to  the original input size.</p></li><li><p><code>p1</code>, <code>p2</code>: padding for the first and third convolution (<code>p1</code>) and the second convolution (<code>p2</code>) in residual block</p></li><li><p><code>s1</code>, <code>s2</code>: stride for the first and third convolution (<code>s1</code>) and the second convolution (<code>s2</code>) in residual block</p></li></ul><p><em>Output</em>:</p><ul><li><code>L</code>: invertible i-RIM network.</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>η_out, s_out = L.forward(η_in, s_in, d, A)</code></p></li><li><p>Inverse mode: <code>η_in, s_in = L.inverse(η_out, s_out, d, A)</code></p></li><li><p>Backward mode: <code>Δη_in, Δs_in, η_in, s_in = L.backward(Δη_out, Δs_out, η_out, s_out, d, A)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>None in <code>L</code> itself</p></li><li><p>Trainable parameters in the invertible coupling layers <code>L.L[i]</code>, and actnorm layers <code>L.AN[i]</code>, where <code>i</code> ranges from <code>1</code> to the number of loop iterations.</p></li></ul><p>See also: <a href="#InvertibleNetworks.CouplingLayerIRIM"><code>CouplingLayerIRIM</code></a>, <a href="#InvertibleNetworks.ResidualBlock"><code>ResidualBlock</code></a>, <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/networks/invertible_network_irim.jl#L7-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.NetworkMultiScaleConditionalHINT" href="#InvertibleNetworks.NetworkMultiScaleConditionalHINT"><code>InvertibleNetworks.NetworkMultiScaleConditionalHINT</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">CH = NetworkMultiScaleConditionalHINT(nx, ny, n_in, batchsize, n_hidden,  L, K; split_scales=false, k1=3, k2=3, p1=1, p2=1, s1=1, s2=1)</code></pre><p>Create a conditional HINT network for data-driven generative modeling based  on the change of variables formula.</p><p><em>Input</em>: </p><ul><li><code>nx</code>, <code>ny</code>, <code>n_in</code>, <code>batchsize</code>: spatial dimensions, number of channels and batchsize of input tensors <code>X</code> and <code>Y</code></li></ul><ul><li><p><code>n_hidden</code>: number of hidden units in residual blocks</p></li><li><p><code>L</code>: number of scales (outer loop)</p></li><li><p><code>K</code>: number of flow steps per scale (inner loop)</p></li><li><p><code>split_scales</code>: if true, split output in half along channel dimension after each scale. Feed one half through the next layers,  while saving the remaining channels for the output.</p></li><li><p><code>k1</code>, <code>k2</code>: kernel size for first and third residual layer (<code>k1</code>) and second layer (<code>k2</code>)</p></li><li><p><code>p1</code>, <code>p2</code>: respective padding sizes for residual block layers</p></li></ul><ul><li><code>s1</code>, <code>s2</code>: respective strides for residual block layers</li></ul><p><em>Output</em>:</p><ul><li><code>CH</code>: conditional HINT network</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Zx, Zy, logdet = CH.forward(X, Y)</code></p></li><li><p>Inverse mode: <code>X, Y = CH.inverse(Zx, Zy)</code></p></li><li><p>Backward mode: <code>ΔX, X = CH.backward(ΔZx, ΔZy, Zx, Zy)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>None in <code>CH</code> itself</p></li><li><p>Trainable parameters in activation normalizations <code>CH.AN_X[i]</code> and <code>CH.AN_Y[i]</code>, </p></li></ul><p>and in coupling layers <code>CH.CL[i]</code>, where <code>i</code> ranges from <code>1</code> to <code>depth</code>.</p><p>See also: <a href="#InvertibleNetworks.ActNorm"><code>ActNorm</code></a>, <a href="@ref"><code>ConditionalLayerHINT!</code></a>, <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/networks/invertible_network_conditional_hint_multiscale.jl#L7-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.NetworkMultiScaleHINT" href="#InvertibleNetworks.NetworkMultiScaleHINT"><code>InvertibleNetworks.NetworkMultiScaleHINT</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">H = NetworkMultiScaleHINT(nx, ny, n_in, batchsize, n_hidden, L, K; split_scales=false, k1=3, k2=3, p1=1, p2=1, s1=1, s2=1)</code></pre><p>Create a multiscale HINT network for data-driven generative modeling based  on the change of variables formula.</p><p><em>Input</em>: </p><ul><li><code>nx</code>, <code>ny</code>, <code>n_in</code>, <code>batchsize</code>: spatial dimensions, number of channels and batchsize of input tensor <code>X</code></li></ul><ul><li><p><code>n_hidden</code>: number of hidden units in residual blocks</p></li><li><p><code>L</code>: number of scales (outer loop)</p></li><li><p><code>K</code>: number of flow steps per scale (inner loop)</p></li><li><p><code>split_scales</code>: if true, split output in half along channel dimension after each scale. Feed one half through the next layers,  while saving the remaining channels for the output.</p></li><li><p><code>k1</code>, <code>k2</code>: kernel size for first and third residual layer (<code>k1</code>) and second layer (<code>k2</code>)</p></li><li><p><code>p1</code>, <code>p2</code>: respective padding sizes for residual block layers</p></li></ul><ul><li><code>s1</code>, <code>s2</code>: respective strides for residual block layers</li></ul><p><em>Output</em>:</p><ul><li><code>H</code>: multiscale HINT network</li></ul><p><em>Usage:</em></p><ul><li><p>Forward mode: <code>Z, logdet = H.forward(X)</code></p></li><li><p>Inverse mode: <code>X = H.inverse(Z)</code></p></li><li><p>Backward mode: <code>ΔX, X = H.backward(ΔZ, Z)</code></p></li></ul><p><em>Trainable parameters:</em></p><ul><li><p>None in <code>H</code> itself</p></li><li><p>Trainable parameters in activation normalizations <code>H.AN[i]</code>, </p></li></ul><p>and in coupling layers <code>H.CL[i]</code>, where <code>i</code> ranges from <code>1</code> to <code>depth</code>.</p><p>See also: <a href="#InvertibleNetworks.ActNorm"><code>ActNorm</code></a>, <a href="@ref"><code>CouplingLayerHINT!</code></a>, <a href="@ref"><code>get_params</code></a>, <a href="@ref"><code>clear_grad!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/networks/invertible_network_hint_multiscale.jl#L7-L52">source</a></section></article><h2 id="Activations-functions"><a class="docs-heading-anchor" href="#Activations-functions">Activations functions</a><a id="Activations-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Activations-functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.ExpClamp-Tuple{Any}" href="#InvertibleNetworks.ExpClamp-Tuple{Any}"><code>InvertibleNetworks.ExpClamp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">y = ExpClamp(x)</code></pre><p>Soft-clamped exponential function.  See also: <a href="@ref"><code>ExpClampGrad</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/activation_functions.jl#L247-L251">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.ExpClampInv-Tuple{Any}" href="#InvertibleNetworks.ExpClampInv-Tuple{Any}"><code>InvertibleNetworks.ExpClampInv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">x = ExpClampInv(y)</code></pre><p>Inverse of ExpClamp function.  See also: <a href="#InvertibleNetworks.ExpClamp-Tuple{Any}"><code>ExpClamp</code></a>, <a href="@ref"><code>ExpClampGrad</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/activation_functions.jl#L256-L260">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.GaLU-Union{Tuple{AbstractArray{Float32,N}}, Tuple{N}} where N" href="#InvertibleNetworks.GaLU-Union{Tuple{AbstractArray{Float32,N}}, Tuple{N}} where N"><code>InvertibleNetworks.GaLU</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">y = GaLU(x)</code></pre><p>Gated linear activation unit (not invertible).</p><p>See also: <a href="#InvertibleNetworks.GaLUgrad-Union{Tuple{N}, Tuple{AbstractArray{Float32,N},AbstractArray{Float32,N}}} where N"><code>GaLUgrad</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/activation_functions.jl#L195-L201">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.GaLUgrad-Union{Tuple{N}, Tuple{AbstractArray{Float32,N},AbstractArray{Float32,N}}} where N" href="#InvertibleNetworks.GaLUgrad-Union{Tuple{N}, Tuple{AbstractArray{Float32,N},AbstractArray{Float32,N}}} where N"><code>InvertibleNetworks.GaLUgrad</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Δx = GaLUgrad(Δy, x)</code></pre><p>Backpropagate data residual through GaLU activation.</p><p><em>Input</em>:</p><ul><li><p><code>Δy</code>: residual</p></li><li><p><code>x</code>: original input (since not invertible)</p></li></ul><p><em>Output</em>:</p><ul><li><code>Δx</code>: backpropagated residual</li></ul><p>See also: <a href="#InvertibleNetworks.GaLU-Union{Tuple{AbstractArray{Float32,N}}, Tuple{N}} where N"><code>GaLU</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/activation_functions.jl#L208-L224">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.LeakyReLU-Tuple{Any}" href="#InvertibleNetworks.LeakyReLU-Tuple{Any}"><code>InvertibleNetworks.LeakyReLU</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">y = LeakyReLU(x; slope=0.01f0)</code></pre><p>Leaky rectified linear unit.</p><p>See also: <a href="#InvertibleNetworks.LeakyReLUinv-Tuple{Any}"><code>LeakyReLUinv</code></a>, <a href="#InvertibleNetworks.LeakyReLUgrad-Tuple{Any,Any}"><code>LeakyReLUgrad</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/activation_functions.jl#L85-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.LeakyReLUgrad-Tuple{Any,Any}" href="#InvertibleNetworks.LeakyReLUgrad-Tuple{Any,Any}"><code>InvertibleNetworks.LeakyReLUgrad</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Δx = ReLUgrad(Δy, y; slope=0.01f0)</code></pre><p>Backpropagate data residual through leaky ReLU function.</p><p><em>Input</em>:</p><ul><li><p><code>Δy</code>: residual</p></li><li><p><code>y</code>: original output</p></li><li><p><code>slope</code>: slope of non-active part of ReLU</p></li></ul><p><em>Output</em>:</p><ul><li><code>Δx</code>: backpropagated residual</li></ul><p>See also: <a href="#InvertibleNetworks.LeakyReLU-Tuple{Any}"><code>LeakyReLU</code></a>, <a href="#InvertibleNetworks.LeakyReLUinv-Tuple{Any}"><code>LeakyReLUinv</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/activation_functions.jl#L107-L125">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.LeakyReLUinv-Tuple{Any}" href="#InvertibleNetworks.LeakyReLUinv-Tuple{Any}"><code>InvertibleNetworks.LeakyReLUinv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">x = LeakyReLUinv(y; slope=0.01f0)</code></pre><p>Inverse of leaky ReLU.</p><p>See also: <a href="#InvertibleNetworks.LeakyReLU-Tuple{Any}"><code>LeakyReLU</code></a>, <a href="#InvertibleNetworks.LeakyReLUgrad-Tuple{Any,Any}"><code>LeakyReLUgrad</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/activation_functions.jl#L96-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.ReLU-Tuple{Any}" href="#InvertibleNetworks.ReLU-Tuple{Any}"><code>InvertibleNetworks.ReLU</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">y = ReLU(x)</code></pre><p>Rectified linear unit (not invertible).</p><p>See also: <a href="#InvertibleNetworks.ReLUgrad-Tuple{Any,Any}"><code>ReLUgrad</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/activation_functions.jl#L50-L56">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.ReLUgrad-Tuple{Any,Any}" href="#InvertibleNetworks.ReLUgrad-Tuple{Any,Any}"><code>InvertibleNetworks.ReLUgrad</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Δx = ReLUgrad(Δy, x)</code></pre><p>Backpropagate data residual through ReLU function.</p><p><em>Input</em>:</p><ul><li><p><code>Δy</code>: data residual</p></li><li><p><code>x</code>: original input (since not invertible)</p></li></ul><p><em>Output</em>:</p><ul><li><code>Δx</code>: backpropagated residual</li></ul><p>See also: <a href="#InvertibleNetworks.ReLU-Tuple{Any}"><code>ReLU</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/activation_functions.jl#L61-L77">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.Sigmoid-Tuple{Any}" href="#InvertibleNetworks.Sigmoid-Tuple{Any}"><code>InvertibleNetworks.Sigmoid</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">y = Sigmoid(x)</code></pre><p>Sigmoid activation function.</p><p>See also: <a href="#InvertibleNetworks.SigmoidInv-Tuple{Any}"><code>SigmoidInv</code></a>, <a href="#InvertibleNetworks.SigmoidGrad-Tuple{Any,Any}"><code>SigmoidGrad</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/activation_functions.jl#L136-L142">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.SigmoidGrad-Tuple{Any,Any}" href="#InvertibleNetworks.SigmoidGrad-Tuple{Any,Any}"><code>InvertibleNetworks.SigmoidGrad</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Δx = SigmoidGrad(Δy, y; x=nothing)</code></pre><p>Backpropagate data residual through Sigmoid function.</p><p><em>Input</em>:</p><ul><li><p><code>Δy</code>: residual</p></li><li><p><code>y</code>: original output</p></li><li><p><code>x</code>: original input, if y not available (in this case, set y=nothing)</p></li></ul><p><em>Output</em>:</p><ul><li><code>Δx</code>: backpropagated residual</li></ul><p>See also: <a href="#InvertibleNetworks.Sigmoid-Tuple{Any}"><code>Sigmoid</code></a>, <a href="#InvertibleNetworks.SigmoidInv-Tuple{Any}"><code>SigmoidInv</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/activation_functions.jl#L164-L182">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.SigmoidInv-Tuple{Any}" href="#InvertibleNetworks.SigmoidInv-Tuple{Any}"><code>InvertibleNetworks.SigmoidInv</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">x = SigmoidInv(y)</code></pre><p>Inverse of Sigmoid function.</p><p>See also: <a href="#InvertibleNetworks.Sigmoid-Tuple{Any}"><code>Sigmoid</code></a>, <a href="#InvertibleNetworks.SigmoidGrad-Tuple{Any,Any}"><code>SigmoidGrad</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/activation_functions.jl#L148-L154">source</a></section></article><h2 id="Dimensions-manipulation"><a class="docs-heading-anchor" href="#Dimensions-manipulation">Dimensions manipulation</a><a id="Dimensions-manipulation-1"></a><a class="docs-heading-anchor-permalink" href="#Dimensions-manipulation" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.Haar_squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T" href="#InvertibleNetworks.Haar_squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>InvertibleNetworks.Haar_squeeze</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Y = Haar_squeeze(X)</code></pre><p>Perform a 1-level channelwise 2D (lifting) Haar transform of X and squeeze output of each  transform into 4 channels (per 1 input channel).</p><p><em>Input</em>:</p><ul><li><code>X</code>: 4D input tensor of dimensions <code>nx</code> x <code>ny</code> x <code>n_channel</code> x <code>batchsize</code></li></ul><p><em>Output</em>:</p><ul><li><code>Y</code>: Reshaped tensor of dimensions <code>nx/2</code> x <code>ny/2</code> x <code>n_channel*4</code> x <code>batchsize</code></li></ul><p>See also: <a href="#InvertibleNetworks.wavelet_unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>wavelet_unsqueeze</code></a>, <a href="@ref"><code>Haar_unsqueeze</code></a>, <a href="@ref"><code>HaarLift</code></a>, <a href="#InvertibleNetworks.squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>squeeze</code></a>, <a href="#InvertibleNetworks.unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>unsqueeze</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/dimensionality_operations.jl#L414-L429">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.Haar_squeeze-Union{Tuple{AbstractArray{T,5}}, Tuple{T}} where T" href="#InvertibleNetworks.Haar_squeeze-Union{Tuple{AbstractArray{T,5}}, Tuple{T}} where T"><code>InvertibleNetworks.Haar_squeeze</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Y = Haar_squeeze(X)</code></pre><p>Perform a 1-level channelwise 3D (lifting) Haar transform of X and squeeze output of each  transform into 8 channels (per 1 input channel).</p><p><em>Input</em>:</p><ul><li><code>X</code>: 5D input tensor of dimensions <code>nx</code> x <code>ny</code> x <code>nz</code> x <code>n_channel</code> x <code>batchsize</code></li></ul><p><em>Output</em>:</p><ul><li><code>Y</code>: Reshaped tensor of dimensions <code>nx/2</code> x <code>ny/2</code> x <code>nz/2</code> x <code>n_channel*8</code> x <code>batchsize</code></li></ul><p>See also: <a href="#InvertibleNetworks.wavelet_unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>wavelet_unsqueeze</code></a>, <a href="@ref"><code>Haar_unsqueeze</code></a>, <a href="@ref"><code>HaarLift</code></a>, <a href="#InvertibleNetworks.squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>squeeze</code></a>, <a href="#InvertibleNetworks.unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>unsqueeze</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/dimensionality_operations.jl#L440-L455">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.invHaar_unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T" href="#InvertibleNetworks.invHaar_unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>InvertibleNetworks.invHaar_unsqueeze</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">X = invHaar_unsqueeze(Y)</code></pre><p>Perform a 1-level inverse 2D Haar transform of Y and unsqueeze output.  This reduces the number of channels by 4 and increases each spatial  dimension by a factor of 2. Inverse operation of <code>Haar_squeeze</code>.</p><p><em>Input</em>:</p><ul><li><code>Y</code>: 4D input tensor of dimensions <code>nx</code> x <code>ny</code> x <code>n_channel</code> x <code>batchsize</code></li></ul><p><em>Output</em>:</p><ul><li><code>X</code>: Reshaped tensor of dimenions <code>nx*2</code> x <code>ny*2</code> x <code>n_channel/4</code> x <code>batchsize</code></li></ul><p>See also: <a href="#InvertibleNetworks.wavelet_unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>wavelet_unsqueeze</code></a>, <a href="#InvertibleNetworks.Haar_squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>Haar_squeeze</code></a>, <a href="@ref"><code>HaarLift</code></a>, <a href="#InvertibleNetworks.squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>squeeze</code></a>, <a href="#InvertibleNetworks.unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>unsqueeze</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/dimensionality_operations.jl#L471-L487">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.invHaar_unsqueeze-Union{Tuple{AbstractArray{T,5}}, Tuple{T}} where T" href="#InvertibleNetworks.invHaar_unsqueeze-Union{Tuple{AbstractArray{T,5}}, Tuple{T}} where T"><code>InvertibleNetworks.invHaar_unsqueeze</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">X = invHaar_unsqueeze(Y)</code></pre><p>Perform a 1-level inverse 3D Haar transform of Y and unsqueeze output.  This reduces the number of channels by 8 and increases each spatial  dimension by a factor of 2. Inverse operation of <code>Haar_squeeze</code>.</p><p><em>Input</em>:</p><ul><li><code>Y</code>: 5D input tensor of dimensions <code>nx</code> x <code>ny</code> x <code>nz</code> x <code>n_channel</code> x <code>batchsize</code></li></ul><p><em>Output</em>:</p><ul><li><code>X</code>: Reshaped tensor of dimenions <code>nx*2</code> x <code>ny*2</code> x <code>nz*2</code> x <code>n_channel/8</code> x <code>batchsize</code></li></ul><p>See also: <a href="#InvertibleNetworks.wavelet_unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>wavelet_unsqueeze</code></a>, <a href="@ref"><code>Haar_unsqueeze</code></a>, <a href="@ref"><code>HaarLift</code></a>, <a href="#InvertibleNetworks.squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>squeeze</code></a>, <a href="#InvertibleNetworks.unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>unsqueeze</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/dimensionality_operations.jl#L505-L521">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T" href="#InvertibleNetworks.squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>InvertibleNetworks.squeeze</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Y = squeeze(X; pattern=&quot;column&quot;)</code></pre><p>Reshape input image such that each spatial dimension is reduced by a factor  of 2, while the number of channels is increased by a factor of 4.</p><p><em>Input</em>:</p><ul><li><p><code>X</code>: 4D input tensor of dimensions <code>nx</code> x <code>ny</code> x <code>n_channel</code> x <code>batchsize</code></p></li><li><p><code>pattern</code>: Squeezing pattern</p><pre><code class="language-none"> 1 2 3 4        1 1 3 3        1 3 1 3
 1 2 3 4        1 1 3 3        2 4 2 4
 1 2 3 4        2 2 4 4        1 3 1 3
 1 2 3 4        2 2 4 4        2 4 2 4

 column          patch       checkerboard</code></pre></li></ul><p><em>Output</em>:</p><ul><li><code>Y</code>: Reshaped tensor of dimensions <code>nx/2</code> x <code>ny/2</code> x <code>n_channel*4</code> x <code>batchsize</code></li></ul><p>See also: <a href="#InvertibleNetworks.unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>unsqueeze</code></a>, <a href="#InvertibleNetworks.wavelet_squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>wavelet_squeeze</code></a>, <a href="#InvertibleNetworks.wavelet_unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>wavelet_unsqueeze</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/dimensionality_operations.jl#L10-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.tensor_cat-Union{Tuple{N}, Tuple{T}, Tuple{AbstractArray{T,N},AbstractArray{T,N}}} where N where T" href="#InvertibleNetworks.tensor_cat-Union{Tuple{N}, Tuple{T}, Tuple{AbstractArray{T,N},AbstractArray{T,N}}} where N where T"><code>InvertibleNetworks.tensor_cat</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">X = tensor_cat(Y, Z)</code></pre><p>Concatenate ND input tensors along the channel dimension. Inverse operation  of <code>tensor_split</code>.</p><p><em>Input</em>:</p><ul><li><code>Y</code>, <code>Z</code>: ND input tensors, each of dimensions <code>nx</code> [x <code>ny</code> [x <code>nz</code>]] x <code>n_channel</code> x <code>batchsize</code></li></ul><p><em>Output</em>:</p><ul><li><code>X</code>: ND output tensor of dimensions <code>nx</code> [x <code>ny</code> [x <code>nz</code>]] x <code>n_channel*2</code> x <code>batchsize</code></li></ul><p>See also: <a href="#InvertibleNetworks.tensor_split-Union{Tuple{AbstractArray{T,N}}, Tuple{N}, Tuple{T}} where N where T"><code>tensor_split</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/dimensionality_operations.jl#L581-L596">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.tensor_split-Union{Tuple{AbstractArray{T,N}}, Tuple{N}, Tuple{T}} where N where T" href="#InvertibleNetworks.tensor_split-Union{Tuple{AbstractArray{T,N}}, Tuple{N}, Tuple{T}} where N where T"><code>InvertibleNetworks.tensor_split</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Y, Z = tensor_split(X)</code></pre><p>Split ND input tensor in half along the channel dimension. Inverse operation  of <code>tensor_cat</code>.</p><p><em>Input</em>:</p><ul><li><code>X</code>: ND input tensor of dimensions <code>nx</code> [x <code>ny</code> [x <code>nz</code>]] x <code>n_channel</code> x <code>batchsize</code></li></ul><p><em>Output</em>:</p><ul><li><code>Y</code>, <code>Z</code>: ND output tensors, each of dimensions <code>nx</code> [x <code>ny</code> [x <code>nz</code>]] x <code>n_channel/2</code> x <code>batchsize</code></li></ul><p>See also: <a href="#InvertibleNetworks.tensor_cat-Union{Tuple{N}, Tuple{T}, Tuple{AbstractArray{T,N},AbstractArray{T,N}}} where N where T"><code>tensor_cat</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/dimensionality_operations.jl#L551-L566">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T" href="#InvertibleNetworks.unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>InvertibleNetworks.unsqueeze</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">X = unsqueeze(Y; pattern=&quot;column&quot;)</code></pre><p>Undo squeezing operation by reshaping input image such that each spatial dimension is  increased by a factor of 2, while the number of channels is decreased by a factor of 4.</p><p><em>Input</em>:</p><ul><li><p><code>Y</code>: 4D input tensor of dimensions <code>nx</code> x <code>ny</code> x <code>n_channel</code> x <code>batchsize</code></p></li><li><p><code>pattern</code>: Squeezing pattern</p><pre><code class="language-none">     1 2 3 4        1 1 3 3        1 3 1 3
     1 2 3 4        1 1 3 3        2 4 2 4
     1 2 3 4        2 2 4 4        1 3 1 3
     1 2 3 4        2 2 4 4        2 4 2 4

     column          patch       checkerboard</code></pre></li></ul><p><em>Output</em>:</p><ul><li><code>X</code>: Reshaped tensor of dimensions <code>nx*2</code> x <code>ny*2</code> x <code>n_channel/4</code> x <code>batchsize</code></li></ul><p>See also: <a href="#InvertibleNetworks.squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>squeeze</code></a>, <a href="#InvertibleNetworks.wavelet_squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>wavelet_squeeze</code></a>, <a href="#InvertibleNetworks.wavelet_unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>wavelet_unsqueeze</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/dimensionality_operations.jl#L98-L122">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.wavelet_squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T" href="#InvertibleNetworks.wavelet_squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>InvertibleNetworks.wavelet_squeeze</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">Y = wavelet_squeeze(X; type=WT.db1)</code></pre><p>Perform a 1-level channelwise 2D wavelet transform of X and squeeze output of each  transform into 4 channels (per 1 input channel).</p><p><em>Input</em>:</p><ul><li><p><code>X</code>: 4D input tensor of dimensions <code>nx</code> x <code>ny</code> x <code>n_channel</code> x <code>batchsize</code></p></li><li><p><code>type</code>: Wavelet filter type. Possible values are <code>WT.haar</code> for Haar wavelets,  <code>WT.coif2</code>, <code>WT.coif4</code>, etc. for Coiflet wavelets, or <code>WT.db1</code>, <code>WT.db2</code>, etc.  for Daubechies wavetlets. See <em>https://github.com/JuliaDSP/Wavelets.jl</em> for a  full list.</p></li></ul><p><em>Output</em>:</p><ul><li><code>Y</code>: Reshaped tensor of dimensions <code>nx/2</code> x <code>ny/2</code> x <code>n_channel*4</code> x <code>batchsize</code></li></ul><p>See also: <a href="#InvertibleNetworks.wavelet_unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>wavelet_unsqueeze</code></a>, <a href="#InvertibleNetworks.squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>squeeze</code></a>, <a href="#InvertibleNetworks.unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>unsqueeze</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/dimensionality_operations.jl#L192-L212">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="InvertibleNetworks.wavelet_unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T" href="#InvertibleNetworks.wavelet_unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>InvertibleNetworks.wavelet_unsqueeze</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">X = wavelet_unsqueeze(Y; type=WT.db1)</code></pre><p>Perform a 1-level inverse 2D wavelet transform of Y and unsqueeze output.  This reduces the number of channels by 4 and increases each spatial  dimension by a factor of 2. Inverse operation of <code>wavelet_squeeze</code>.</p><p><em>Input</em>:</p><ul><li><p><code>Y</code>: 4D input tensor of dimensions <code>nx</code> x <code>ny</code> x <code>n_channel</code> x <code>batchsize</code></p></li><li><p><code>type</code>: Wavelet filter type. Possible values are <code>haar</code> for Haar wavelets,</p></li></ul><p><code>coif2</code>, <code>coif4</code>, etc. for Coiflet wavelets, or <code>db1</code>, <code>db2</code>, etc. for Daubechies   wavetlets. See <em>https://github.com/JuliaDSP/Wavelets.jl</em> for a full list.</p><p><em>Output</em>:</p><ul><li><code>X</code>: Reshaped tensor of dimenions <code>nx*2</code> x <code>ny*2</code> x <code>n_channel/4</code> x <code>batchsize</code></li></ul><p>See also: <a href="#InvertibleNetworks.wavelet_squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>wavelet_squeeze</code></a>, <a href="#InvertibleNetworks.squeeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>squeeze</code></a>, <a href="#InvertibleNetworks.unsqueeze-Union{Tuple{AbstractArray{T,4}}, Tuple{T}} where T"><code>unsqueeze</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/slimgroup/InvertibleNetworks.jl/blob/ee8c395e186278db5ae611fce8547a3c51cb7c1c/src/utils/dimensionality_operations.jl#L244-L264">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><a class="docs-footer-nextpage" href="../LICENSE/">LICENSE »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 24 February 2021 21:10">Wednesday 24 February 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
